{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "834ed915-a1b1-48cd-84fa-7984ada4edfd",
      "metadata": {
        "id": "834ed915-a1b1-48cd-84fa-7984ada4edfd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.models import resnet50\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "# Initialize Inception model\n",
        "import torchvision.models as models\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pennylane"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "676fATC5qmSG",
        "outputId": "b42294a6-08d9-4654-e83b-945520a3c0cf"
      },
      "id": "676fATC5qmSG",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pennylane\n",
            "  Downloading PennyLane-0.36.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.5/1.7 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0 in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.11.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pennylane) (3.3)\n",
            "Collecting rustworkx (from pennylane)\n",
            "  Downloading rustworkx-0.14.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: autograd in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.6.2)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from pennylane) (0.10.2)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.4.4)\n",
            "Collecting semantic-version>=2.7 (from pennylane)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting autoray>=0.6.1 (from pennylane)\n",
            "  Downloading autoray-0.6.10-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from pennylane) (5.3.3)\n",
            "Collecting pennylane-lightning>=0.36 (from pennylane)\n",
            "  Downloading PennyLane_Lightning-0.36.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pennylane) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pennylane) (4.11.0)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.10/dist-packages (from autograd->pennylane) (0.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2024.2.2)\n",
            "Installing collected packages: semantic-version, rustworkx, autoray, pennylane-lightning, pennylane\n",
            "Successfully installed autoray-0.6.10 pennylane-0.36.0 pennylane-lightning-0.36.0 rustworkx-0.14.2 semantic-version-2.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0df93d42-2626-4472-bb97-753aee657532",
      "metadata": {
        "id": "0df93d42-2626-4472-bb97-753aee657532"
      },
      "outputs": [],
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a426ba89-50f5-49db-a0ee-984905f9d58c",
      "metadata": {
        "id": "a426ba89-50f5-49db-a0ee-984905f9d58c"
      },
      "outputs": [],
      "source": [
        "# Data preprocessing\n",
        "# Resize images to 299x299\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((299, 299)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89d49e1a-23a3-478e-b2ad-222f257e602d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89d49e1a-23a3-478e-b2ad-222f257e602d",
        "outputId": "bec84ac4-cdb8-4adf-cc9c-0431d484cf77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:03<00:00, 48968265.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "# Load CIFAR-10 dataset\n",
        "train_dataset = CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True, num_workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52bc20bd-8a24-4870-a52b-b1b3201d083e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52bc20bd-8a24-4870-a52b-b1b3201d083e",
        "outputId": "63f4ed9d-159b-4cec-fbc3-2c53aa0e9d71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n",
            "100%|██████████| 104M/104M [00:01<00:00, 56.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Initialize ResNet50 model\n",
        "import torchvision.models as models\n",
        "model = models.inception_v3(pretrained=True)\n",
        "num_classes = 10\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "687a2b93-5952-413a-b38a-39a61eb1e37c",
      "metadata": {
        "id": "687a2b93-5952-413a-b38a-39a61eb1e37c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pennylane as qml\n",
        "\n",
        "dev = qml.device(\"default.qubit\", wires=4)\n",
        "\n",
        "@qml.qnode(dev)\n",
        "def quantum_net(q_input_features, q_weights_flat):\n",
        "    # Reshape weights\n",
        "    q_weights = q_weights_flat.reshape((4, 6))  # Assuming 6 parameters per qubit\n",
        "\n",
        "    # Quantum circuit\n",
        "    for i in range(4):\n",
        "        qml.Rot(q_input_features[i][0], q_input_features[i][1], q_input_features[i][2], wires=i)\n",
        "        qml.RX(q_weights[i][0], wires=i)\n",
        "        qml.RY(q_weights[i][1], wires=i)\n",
        "        qml.RZ(q_weights[i][2], wires=i)\n",
        "        qml.RX(q_weights[i][3], wires=i)  # Additional gate\n",
        "        qml.RY(q_weights[i][4], wires=i)  # Additional gate\n",
        "        qml.RZ(q_weights[i][5], wires=i)  # Additional gate\n",
        "\n",
        "    return [qml.expval(qml.PauliZ(i)) for i in range(4)]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac04cf25-32a2-4f2e-aed5-052d12afe08a",
      "metadata": {
        "id": "ac04cf25-32a2-4f2e-aed5-052d12afe08a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\"\"\"\n",
        "\n",
        "class QuantumNetLayer(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(input_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Assuming the input_size and output_size are known\n",
        "quantum_layer = QuantumNetLayer(input_size=2048, output_size=10)  # Assuming output_size for CIFAR-10 is 10\n",
        "\"\"\"\n",
        "# Inside the QuantumNetLayer class\n",
        "class QuantumNetLayer(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super().__init__()\n",
        "        self.q_weights_flat = nn.Parameter(torch.randn(4, 6).to(device))  # Define trainable quantum weights\n",
        "        self.fc = nn.Linear(4, output_size).to(device)  # Adjust the input size to match the output of the quantum circuit\n",
        "\n",
        "    def forward(self, x):\n",
        "        q_features = quantum_net(torch.flatten(x, start_dim=1), self.q_weights_flat)\n",
        "        q_features_tensor = torch.tensor(q_features, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "        x = self.fc(q_features_tensor)\n",
        "        return x\n",
        "\n",
        "# Assuming the output_size is known\n",
        "quantum_layer = QuantumNetLayer(input_size=2048, output_size=10)  # Adjust the input size to match the output of the quantum circuit\n",
        "\n",
        "# Replace the fully connected layer in the ResNet model with the QuantumNetLayer\n",
        "model.fc = quantum_layer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8d7d617-90a5-4228-872a-81d8a9f808d8",
      "metadata": {
        "id": "f8d7d617-90a5-4228-872a-81d8a9f808d8"
      },
      "outputs": [],
      "source": [
        "# Set the device (GPU or CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17e45f1d-debd-4329-bf47-bf2017e0c5ec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17e45f1d-debd-4329-bf47-bf2017e0c5ec",
        "outputId": "c7660f63-8050-42ab-e9d9-f66cad9c56ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inception3(\n",
            "  (Conv2d_1a_3x3): BasicConv2d(\n",
            "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
            "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (Conv2d_2a_3x3): BasicConv2d(\n",
            "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (Conv2d_2b_3x3): BasicConv2d(\n",
            "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (Conv2d_3b_1x1): BasicConv2d(\n",
            "    (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (Conv2d_4a_3x3): BasicConv2d(\n",
            "    (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "    (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (Mixed_5b): InceptionA(\n",
            "    (branch1x1): BasicConv2d(\n",
            "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch5x5_1): BasicConv2d(\n",
            "      (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch5x5_2): BasicConv2d(\n",
            "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_1): BasicConv2d(\n",
            "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_2): BasicConv2d(\n",
            "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_3): BasicConv2d(\n",
            "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch_pool): BasicConv2d(\n",
            "      (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (Mixed_5c): InceptionA(\n",
            "    (branch1x1): BasicConv2d(\n",
            "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch5x5_1): BasicConv2d(\n",
            "      (conv): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch5x5_2): BasicConv2d(\n",
            "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_1): BasicConv2d(\n",
            "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_2): BasicConv2d(\n",
            "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_3): BasicConv2d(\n",
            "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch_pool): BasicConv2d(\n",
            "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (Mixed_5d): InceptionA(\n",
            "    (branch1x1): BasicConv2d(\n",
            "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch5x5_1): BasicConv2d(\n",
            "      (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch5x5_2): BasicConv2d(\n",
            "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_1): BasicConv2d(\n",
            "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_2): BasicConv2d(\n",
            "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_3): BasicConv2d(\n",
            "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch_pool): BasicConv2d(\n",
            "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (Mixed_6a): InceptionB(\n",
            "    (branch3x3): BasicConv2d(\n",
            "      (conv): Conv2d(288, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_1): BasicConv2d(\n",
            "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_2): BasicConv2d(\n",
            "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_3): BasicConv2d(\n",
            "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
            "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (Mixed_6b): InceptionC(\n",
            "    (branch1x1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_2): BasicConv2d(\n",
            "      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_3): BasicConv2d(\n",
            "      (conv): Conv2d(128, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_2): BasicConv2d(\n",
            "      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_3): BasicConv2d(\n",
            "      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_4): BasicConv2d(\n",
            "      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_5): BasicConv2d(\n",
            "      (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch_pool): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (Mixed_6c): InceptionC(\n",
            "    (branch1x1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_2): BasicConv2d(\n",
            "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_3): BasicConv2d(\n",
            "      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_2): BasicConv2d(\n",
            "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_3): BasicConv2d(\n",
            "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_4): BasicConv2d(\n",
            "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_5): BasicConv2d(\n",
            "      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch_pool): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (Mixed_6d): InceptionC(\n",
            "    (branch1x1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_2): BasicConv2d(\n",
            "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_3): BasicConv2d(\n",
            "      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_2): BasicConv2d(\n",
            "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_3): BasicConv2d(\n",
            "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_4): BasicConv2d(\n",
            "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_5): BasicConv2d(\n",
            "      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch_pool): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (Mixed_6e): InceptionC(\n",
            "    (branch1x1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_2): BasicConv2d(\n",
            "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_3): BasicConv2d(\n",
            "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_2): BasicConv2d(\n",
            "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_3): BasicConv2d(\n",
            "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_4): BasicConv2d(\n",
            "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_5): BasicConv2d(\n",
            "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch_pool): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (AuxLogits): InceptionAux(\n",
            "    (conv0): BasicConv2d(\n",
            "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (conv1): BasicConv2d(\n",
            "      (conv): Conv2d(128, 768, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (fc): Linear(in_features=768, out_features=1000, bias=True)\n",
            "  )\n",
            "  (Mixed_7a): InceptionD(\n",
            "    (branch3x3_1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3_2): BasicConv2d(\n",
            "      (conv): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
            "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7x3_1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7x3_2): BasicConv2d(\n",
            "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7x3_3): BasicConv2d(\n",
            "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7x3_4): BasicConv2d(\n",
            "      (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (Mixed_7b): InceptionE(\n",
            "    (branch1x1): BasicConv2d(\n",
            "      (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3_1): BasicConv2d(\n",
            "      (conv): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3_2a): BasicConv2d(\n",
            "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3_2b): BasicConv2d(\n",
            "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_1): BasicConv2d(\n",
            "      (conv): Conv2d(1280, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_2): BasicConv2d(\n",
            "      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_3a): BasicConv2d(\n",
            "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_3b): BasicConv2d(\n",
            "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch_pool): BasicConv2d(\n",
            "      (conv): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (Mixed_7c): InceptionE(\n",
            "    (branch1x1): BasicConv2d(\n",
            "      (conv): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3_1): BasicConv2d(\n",
            "      (conv): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3_2a): BasicConv2d(\n",
            "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3_2b): BasicConv2d(\n",
            "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_1): BasicConv2d(\n",
            "      (conv): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_2): BasicConv2d(\n",
            "      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_3a): BasicConv2d(\n",
            "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_3b): BasicConv2d(\n",
            "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch_pool): BasicConv2d(\n",
            "      (conv): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): QuantumNetLayer(\n",
            "    (fc): Linear(in_features=4, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fe0502a-c6ec-4c21-b750-1b19bb19326b",
      "metadata": {
        "id": "3fe0502a-c6ec-4c21-b750-1b19bb19326b"
      },
      "outputs": [],
      "source": [
        "# Loss function and optimizer\n",
        "import torch.optim as optim\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "#optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "#scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a6e24e6-87b5-4179-9866-3b776245482a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a6e24e6-87b5-4179-9866-3b776245482a",
        "outputId": "dd6f34a5-d641-40c4-b35f-580daade8eac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Step [1/782], Loss: 5.539053916931152\n",
            "Epoch [1/5], Step [2/782], Loss: 5.2971649169921875\n",
            "Epoch [1/5], Step [3/782], Loss: 5.078428268432617\n",
            "Epoch [1/5], Step [4/782], Loss: 4.835585117340088\n",
            "Epoch [1/5], Step [5/782], Loss: 4.63450813293457\n",
            "Epoch [1/5], Step [6/782], Loss: 4.398934364318848\n",
            "Epoch [1/5], Step [7/782], Loss: 4.4112114906311035\n",
            "Epoch [1/5], Step [8/782], Loss: 4.243680953979492\n",
            "Epoch [1/5], Step [9/782], Loss: 4.286917209625244\n",
            "Epoch [1/5], Step [10/782], Loss: 4.920343399047852\n",
            "Epoch [1/5], Step [11/782], Loss: 4.409223556518555\n",
            "Epoch [1/5], Step [12/782], Loss: 4.631003379821777\n",
            "Epoch [1/5], Step [13/782], Loss: 4.644822120666504\n",
            "Epoch [1/5], Step [14/782], Loss: 4.722283363342285\n",
            "Epoch [1/5], Step [15/782], Loss: 4.825125694274902\n",
            "Epoch [1/5], Step [16/782], Loss: 4.596438884735107\n",
            "Epoch [1/5], Step [17/782], Loss: 4.445266246795654\n",
            "Epoch [1/5], Step [18/782], Loss: 4.789846420288086\n",
            "Epoch [1/5], Step [19/782], Loss: 4.535282135009766\n",
            "Epoch [1/5], Step [20/782], Loss: 4.4164886474609375\n",
            "Epoch [1/5], Step [21/782], Loss: 4.323115348815918\n",
            "Epoch [1/5], Step [22/782], Loss: 4.341552734375\n",
            "Epoch [1/5], Step [23/782], Loss: 4.549139499664307\n",
            "Epoch [1/5], Step [24/782], Loss: 4.345571517944336\n",
            "Epoch [1/5], Step [25/782], Loss: 4.309374809265137\n",
            "Epoch [1/5], Step [26/782], Loss: 4.4792046546936035\n",
            "Epoch [1/5], Step [27/782], Loss: 4.472169876098633\n",
            "Epoch [1/5], Step [28/782], Loss: 4.29531192779541\n",
            "Epoch [1/5], Step [29/782], Loss: 4.288124084472656\n",
            "Epoch [1/5], Step [30/782], Loss: 4.336978435516357\n",
            "Epoch [1/5], Step [31/782], Loss: 4.103837966918945\n",
            "Epoch [1/5], Step [32/782], Loss: 4.336498260498047\n",
            "Epoch [1/5], Step [33/782], Loss: 4.382996559143066\n",
            "Epoch [1/5], Step [34/782], Loss: 4.00697660446167\n",
            "Epoch [1/5], Step [35/782], Loss: 4.285102844238281\n",
            "Epoch [1/5], Step [36/782], Loss: 4.339095592498779\n",
            "Epoch [1/5], Step [37/782], Loss: 4.489965438842773\n",
            "Epoch [1/5], Step [38/782], Loss: 4.3773088455200195\n",
            "Epoch [1/5], Step [39/782], Loss: 4.258943557739258\n",
            "Epoch [1/5], Step [40/782], Loss: 4.361021518707275\n",
            "Epoch [1/5], Step [41/782], Loss: 4.171576023101807\n",
            "Epoch [1/5], Step [42/782], Loss: 4.071409702301025\n",
            "Epoch [1/5], Step [43/782], Loss: 4.011463642120361\n",
            "Epoch [1/5], Step [44/782], Loss: 4.357669830322266\n",
            "Epoch [1/5], Step [45/782], Loss: 3.949951648712158\n",
            "Epoch [1/5], Step [46/782], Loss: 4.091981410980225\n",
            "Epoch [1/5], Step [47/782], Loss: 4.0637712478637695\n",
            "Epoch [1/5], Step [48/782], Loss: 4.195590496063232\n",
            "Epoch [1/5], Step [49/782], Loss: 4.0212531089782715\n",
            "Epoch [1/5], Step [50/782], Loss: 4.0639424324035645\n",
            "Epoch [1/5], Step [51/782], Loss: 4.056698322296143\n",
            "Epoch [1/5], Step [52/782], Loss: 3.831468343734741\n",
            "Epoch [1/5], Step [53/782], Loss: 4.12645149230957\n",
            "Epoch [1/5], Step [54/782], Loss: 4.014768123626709\n",
            "Epoch [1/5], Step [55/782], Loss: 3.9927172660827637\n",
            "Epoch [1/5], Step [56/782], Loss: 4.120515823364258\n",
            "Epoch [1/5], Step [57/782], Loss: 3.9407851696014404\n",
            "Epoch [1/5], Step [58/782], Loss: 3.918330192565918\n",
            "Epoch [1/5], Step [59/782], Loss: 4.137904167175293\n",
            "Epoch [1/5], Step [60/782], Loss: 4.116209506988525\n",
            "Epoch [1/5], Step [61/782], Loss: 4.18988037109375\n",
            "Epoch [1/5], Step [62/782], Loss: 4.230296611785889\n",
            "Epoch [1/5], Step [63/782], Loss: 4.201085090637207\n",
            "Epoch [1/5], Step [64/782], Loss: 4.12031364440918\n",
            "Epoch [1/5], Step [65/782], Loss: 4.108904838562012\n",
            "Epoch [1/5], Step [66/782], Loss: 3.9400360584259033\n",
            "Epoch [1/5], Step [67/782], Loss: 4.121755599975586\n",
            "Epoch [1/5], Step [68/782], Loss: 4.112112998962402\n",
            "Epoch [1/5], Step [69/782], Loss: 4.271900653839111\n",
            "Epoch [1/5], Step [70/782], Loss: 4.069516658782959\n",
            "Epoch [1/5], Step [71/782], Loss: 3.853461503982544\n",
            "Epoch [1/5], Step [72/782], Loss: 3.9631733894348145\n",
            "Epoch [1/5], Step [73/782], Loss: 3.633226156234741\n",
            "Epoch [1/5], Step [74/782], Loss: 4.317737102508545\n",
            "Epoch [1/5], Step [75/782], Loss: 3.8248658180236816\n",
            "Epoch [1/5], Step [76/782], Loss: 3.9885754585266113\n",
            "Epoch [1/5], Step [77/782], Loss: 3.9175944328308105\n",
            "Epoch [1/5], Step [78/782], Loss: 3.9997167587280273\n",
            "Epoch [1/5], Step [79/782], Loss: 3.827249526977539\n",
            "Epoch [1/5], Step [80/782], Loss: 3.929932117462158\n",
            "Epoch [1/5], Step [81/782], Loss: 3.868100166320801\n",
            "Epoch [1/5], Step [82/782], Loss: 3.839268684387207\n",
            "Epoch [1/5], Step [83/782], Loss: 3.6143016815185547\n",
            "Epoch [1/5], Step [84/782], Loss: 3.882841110229492\n",
            "Epoch [1/5], Step [85/782], Loss: 3.9135539531707764\n",
            "Epoch [1/5], Step [86/782], Loss: 3.729663372039795\n",
            "Epoch [1/5], Step [87/782], Loss: 3.7435503005981445\n",
            "Epoch [1/5], Step [88/782], Loss: 3.502587080001831\n",
            "Epoch [1/5], Step [89/782], Loss: 3.8155784606933594\n",
            "Epoch [1/5], Step [90/782], Loss: 3.859257698059082\n",
            "Epoch [1/5], Step [91/782], Loss: 3.668700933456421\n",
            "Epoch [1/5], Step [92/782], Loss: 3.9181320667266846\n",
            "Epoch [1/5], Step [93/782], Loss: 3.843344211578369\n",
            "Epoch [1/5], Step [94/782], Loss: 4.045323848724365\n",
            "Epoch [1/5], Step [95/782], Loss: 3.8207292556762695\n",
            "Epoch [1/5], Step [96/782], Loss: 4.0624165534973145\n",
            "Epoch [1/5], Step [97/782], Loss: 3.7142539024353027\n",
            "Epoch [1/5], Step [98/782], Loss: 4.039335250854492\n",
            "Epoch [1/5], Step [99/782], Loss: 4.123292446136475\n",
            "Epoch [1/5], Step [100/782], Loss: 3.7829060554504395\n",
            "Epoch [1/5], Step [101/782], Loss: 3.6559369564056396\n",
            "Epoch [1/5], Step [102/782], Loss: 3.7625162601470947\n",
            "Epoch [1/5], Step [103/782], Loss: 3.767303705215454\n",
            "Epoch [1/5], Step [104/782], Loss: 3.715481996536255\n",
            "Epoch [1/5], Step [105/782], Loss: 3.831017017364502\n",
            "Epoch [1/5], Step [106/782], Loss: 3.982302665710449\n",
            "Epoch [1/5], Step [107/782], Loss: 3.7376365661621094\n",
            "Epoch [1/5], Step [108/782], Loss: 3.6987409591674805\n",
            "Epoch [1/5], Step [109/782], Loss: 3.682076930999756\n",
            "Epoch [1/5], Step [110/782], Loss: 3.8972620964050293\n",
            "Epoch [1/5], Step [111/782], Loss: 3.711472511291504\n",
            "Epoch [1/5], Step [112/782], Loss: 3.791410446166992\n",
            "Epoch [1/5], Step [113/782], Loss: 3.699991226196289\n",
            "Epoch [1/5], Step [114/782], Loss: 3.6272497177124023\n",
            "Epoch [1/5], Step [115/782], Loss: 3.6442618370056152\n",
            "Epoch [1/5], Step [116/782], Loss: 3.660010814666748\n",
            "Epoch [1/5], Step [117/782], Loss: 3.900938034057617\n",
            "Epoch [1/5], Step [118/782], Loss: 3.913471221923828\n",
            "Epoch [1/5], Step [119/782], Loss: 3.8086929321289062\n",
            "Epoch [1/5], Step [120/782], Loss: 3.639497756958008\n",
            "Epoch [1/5], Step [121/782], Loss: 3.4986658096313477\n",
            "Epoch [1/5], Step [122/782], Loss: 3.689704418182373\n",
            "Epoch [1/5], Step [123/782], Loss: 3.9644949436187744\n",
            "Epoch [1/5], Step [124/782], Loss: 3.6761302947998047\n",
            "Epoch [1/5], Step [125/782], Loss: 3.967287540435791\n",
            "Epoch [1/5], Step [126/782], Loss: 3.6672749519348145\n",
            "Epoch [1/5], Step [127/782], Loss: 3.7143938541412354\n",
            "Epoch [1/5], Step [128/782], Loss: 3.6471786499023438\n",
            "Epoch [1/5], Step [129/782], Loss: 3.6165082454681396\n",
            "Epoch [1/5], Step [130/782], Loss: 3.6274161338806152\n",
            "Epoch [1/5], Step [131/782], Loss: 3.6852593421936035\n",
            "Epoch [1/5], Step [132/782], Loss: 3.791116714477539\n",
            "Epoch [1/5], Step [133/782], Loss: 3.718745231628418\n",
            "Epoch [1/5], Step [134/782], Loss: 3.653532028198242\n",
            "Epoch [1/5], Step [135/782], Loss: 3.6024718284606934\n",
            "Epoch [1/5], Step [136/782], Loss: 3.8453588485717773\n",
            "Epoch [1/5], Step [137/782], Loss: 3.6403732299804688\n",
            "Epoch [1/5], Step [138/782], Loss: 3.756288528442383\n",
            "Epoch [1/5], Step [139/782], Loss: 3.762812614440918\n",
            "Epoch [1/5], Step [140/782], Loss: 3.4415996074676514\n",
            "Epoch [1/5], Step [141/782], Loss: 3.4972124099731445\n",
            "Epoch [1/5], Step [142/782], Loss: 3.7118163108825684\n",
            "Epoch [1/5], Step [143/782], Loss: 3.8395485877990723\n",
            "Epoch [1/5], Step [144/782], Loss: 3.559103488922119\n",
            "Epoch [1/5], Step [145/782], Loss: 3.8443117141723633\n",
            "Epoch [1/5], Step [146/782], Loss: 3.6349844932556152\n",
            "Epoch [1/5], Step [147/782], Loss: 3.6049256324768066\n",
            "Epoch [1/5], Step [148/782], Loss: 3.4170851707458496\n",
            "Epoch [1/5], Step [149/782], Loss: 3.5894715785980225\n",
            "Epoch [1/5], Step [150/782], Loss: 3.851057529449463\n",
            "Epoch [1/5], Step [151/782], Loss: 3.457200288772583\n",
            "Epoch [1/5], Step [152/782], Loss: 3.543409824371338\n",
            "Epoch [1/5], Step [153/782], Loss: 3.403386116027832\n",
            "Epoch [1/5], Step [154/782], Loss: 3.338433027267456\n",
            "Epoch [1/5], Step [155/782], Loss: 3.4811480045318604\n",
            "Epoch [1/5], Step [156/782], Loss: 3.312119960784912\n",
            "Epoch [1/5], Step [157/782], Loss: 3.7127413749694824\n",
            "Epoch [1/5], Step [158/782], Loss: 3.3068816661834717\n",
            "Epoch [1/5], Step [159/782], Loss: 3.5270345211029053\n",
            "Epoch [1/5], Step [160/782], Loss: 3.8020262718200684\n",
            "Epoch [1/5], Step [161/782], Loss: 3.7097134590148926\n",
            "Epoch [1/5], Step [162/782], Loss: 3.2467548847198486\n",
            "Epoch [1/5], Step [163/782], Loss: 3.639010190963745\n",
            "Epoch [1/5], Step [164/782], Loss: 3.4244070053100586\n",
            "Epoch [1/5], Step [165/782], Loss: 3.753305673599243\n",
            "Epoch [1/5], Step [166/782], Loss: 3.6375925540924072\n",
            "Epoch [1/5], Step [167/782], Loss: 3.5174314975738525\n",
            "Epoch [1/5], Step [168/782], Loss: 3.548271417617798\n",
            "Epoch [1/5], Step [169/782], Loss: 3.501837968826294\n",
            "Epoch [1/5], Step [170/782], Loss: 3.7559142112731934\n",
            "Epoch [1/5], Step [171/782], Loss: 3.6509833335876465\n",
            "Epoch [1/5], Step [172/782], Loss: 3.4524526596069336\n",
            "Epoch [1/5], Step [173/782], Loss: 3.6642565727233887\n",
            "Epoch [1/5], Step [174/782], Loss: 3.650372266769409\n",
            "Epoch [1/5], Step [175/782], Loss: 3.5557374954223633\n",
            "Epoch [1/5], Step [176/782], Loss: 3.303536891937256\n",
            "Epoch [1/5], Step [177/782], Loss: 3.497076988220215\n",
            "Epoch [1/5], Step [178/782], Loss: 3.480407238006592\n",
            "Epoch [1/5], Step [179/782], Loss: 3.5469603538513184\n",
            "Epoch [1/5], Step [180/782], Loss: 3.6429686546325684\n",
            "Epoch [1/5], Step [181/782], Loss: 3.5195202827453613\n",
            "Epoch [1/5], Step [182/782], Loss: 3.604501724243164\n",
            "Epoch [1/5], Step [183/782], Loss: 3.488900899887085\n",
            "Epoch [1/5], Step [184/782], Loss: 3.4885478019714355\n",
            "Epoch [1/5], Step [185/782], Loss: 3.4578499794006348\n",
            "Epoch [1/5], Step [186/782], Loss: 3.3273985385894775\n",
            "Epoch [1/5], Step [187/782], Loss: 3.523536205291748\n",
            "Epoch [1/5], Step [188/782], Loss: 3.6092171669006348\n",
            "Epoch [1/5], Step [189/782], Loss: 3.491611957550049\n",
            "Epoch [1/5], Step [190/782], Loss: 3.547914505004883\n",
            "Epoch [1/5], Step [191/782], Loss: 3.6093544960021973\n",
            "Epoch [1/5], Step [192/782], Loss: 3.4614899158477783\n",
            "Epoch [1/5], Step [193/782], Loss: 3.238584280014038\n",
            "Epoch [1/5], Step [194/782], Loss: 3.2414259910583496\n",
            "Epoch [1/5], Step [195/782], Loss: 3.6303648948669434\n",
            "Epoch [1/5], Step [196/782], Loss: 3.524057626724243\n",
            "Epoch [1/5], Step [197/782], Loss: 3.430251359939575\n",
            "Epoch [1/5], Step [198/782], Loss: 3.4148573875427246\n",
            "Epoch [1/5], Step [199/782], Loss: 3.4231536388397217\n",
            "Epoch [1/5], Step [200/782], Loss: 3.3236632347106934\n",
            "Epoch [1/5], Step [201/782], Loss: 3.414189338684082\n",
            "Epoch [1/5], Step [202/782], Loss: 3.712063789367676\n",
            "Epoch [1/5], Step [203/782], Loss: 3.1167397499084473\n",
            "Epoch [1/5], Step [204/782], Loss: 3.413395881652832\n",
            "Epoch [1/5], Step [205/782], Loss: 3.3208796977996826\n",
            "Epoch [1/5], Step [206/782], Loss: 3.333190441131592\n",
            "Epoch [1/5], Step [207/782], Loss: 3.675593376159668\n",
            "Epoch [1/5], Step [208/782], Loss: 3.348024845123291\n",
            "Epoch [1/5], Step [209/782], Loss: 3.5842721462249756\n",
            "Epoch [1/5], Step [210/782], Loss: 3.415031909942627\n",
            "Epoch [1/5], Step [211/782], Loss: 3.377565383911133\n",
            "Epoch [1/5], Step [212/782], Loss: 3.7801828384399414\n",
            "Epoch [1/5], Step [213/782], Loss: 3.4941391944885254\n",
            "Epoch [1/5], Step [214/782], Loss: 3.4673709869384766\n",
            "Epoch [1/5], Step [215/782], Loss: 3.6219780445098877\n",
            "Epoch [1/5], Step [216/782], Loss: 3.147245407104492\n",
            "Epoch [1/5], Step [217/782], Loss: 3.436682939529419\n",
            "Epoch [1/5], Step [218/782], Loss: 3.2096619606018066\n",
            "Epoch [1/5], Step [219/782], Loss: 3.4198598861694336\n",
            "Epoch [1/5], Step [220/782], Loss: 3.357069730758667\n",
            "Epoch [1/5], Step [221/782], Loss: 3.5921425819396973\n",
            "Epoch [1/5], Step [222/782], Loss: 3.2390918731689453\n",
            "Epoch [1/5], Step [223/782], Loss: 3.421598196029663\n",
            "Epoch [1/5], Step [224/782], Loss: 3.3242790699005127\n",
            "Epoch [1/5], Step [225/782], Loss: 3.258967638015747\n",
            "Epoch [1/5], Step [226/782], Loss: 3.27915620803833\n",
            "Epoch [1/5], Step [227/782], Loss: 3.5547900199890137\n",
            "Epoch [1/5], Step [228/782], Loss: 3.5839858055114746\n",
            "Epoch [1/5], Step [229/782], Loss: 3.3883838653564453\n",
            "Epoch [1/5], Step [230/782], Loss: 3.4579241275787354\n",
            "Epoch [1/5], Step [231/782], Loss: 3.4269771575927734\n",
            "Epoch [1/5], Step [232/782], Loss: 3.4308979511260986\n",
            "Epoch [1/5], Step [233/782], Loss: 3.195213556289673\n",
            "Epoch [1/5], Step [234/782], Loss: 3.3686599731445312\n",
            "Epoch [1/5], Step [235/782], Loss: 3.2512564659118652\n",
            "Epoch [1/5], Step [236/782], Loss: 3.4643826484680176\n",
            "Epoch [1/5], Step [237/782], Loss: 3.391453981399536\n",
            "Epoch [1/5], Step [238/782], Loss: 3.3339853286743164\n",
            "Epoch [1/5], Step [239/782], Loss: 3.2398576736450195\n",
            "Epoch [1/5], Step [240/782], Loss: 3.2915844917297363\n",
            "Epoch [1/5], Step [241/782], Loss: 3.393920660018921\n",
            "Epoch [1/5], Step [242/782], Loss: 3.6746973991394043\n",
            "Epoch [1/5], Step [243/782], Loss: 3.3202619552612305\n",
            "Epoch [1/5], Step [244/782], Loss: 3.246732234954834\n",
            "Epoch [1/5], Step [245/782], Loss: 3.008417844772339\n",
            "Epoch [1/5], Step [246/782], Loss: 3.288360118865967\n",
            "Epoch [1/5], Step [247/782], Loss: 3.220317840576172\n",
            "Epoch [1/5], Step [248/782], Loss: 3.2203454971313477\n",
            "Epoch [1/5], Step [249/782], Loss: 3.1949052810668945\n",
            "Epoch [1/5], Step [250/782], Loss: 3.3979978561401367\n",
            "Epoch [1/5], Step [251/782], Loss: 3.1434693336486816\n",
            "Epoch [1/5], Step [252/782], Loss: 3.180154323577881\n",
            "Epoch [1/5], Step [253/782], Loss: 3.4104485511779785\n",
            "Epoch [1/5], Step [254/782], Loss: 3.2738800048828125\n",
            "Epoch [1/5], Step [255/782], Loss: 3.189058542251587\n",
            "Epoch [1/5], Step [256/782], Loss: 3.2736568450927734\n",
            "Epoch [1/5], Step [257/782], Loss: 3.2847981452941895\n",
            "Epoch [1/5], Step [258/782], Loss: 3.264738082885742\n",
            "Epoch [1/5], Step [259/782], Loss: 3.376027822494507\n",
            "Epoch [1/5], Step [260/782], Loss: 3.083500862121582\n",
            "Epoch [1/5], Step [261/782], Loss: 3.206338882446289\n",
            "Epoch [1/5], Step [262/782], Loss: 3.221982002258301\n",
            "Epoch [1/5], Step [263/782], Loss: 3.2478113174438477\n",
            "Epoch [1/5], Step [264/782], Loss: 3.029754638671875\n",
            "Epoch [1/5], Step [265/782], Loss: 3.5164759159088135\n",
            "Epoch [1/5], Step [266/782], Loss: 3.3951704502105713\n",
            "Epoch [1/5], Step [267/782], Loss: 3.1243581771850586\n",
            "Epoch [1/5], Step [268/782], Loss: 3.395260810852051\n",
            "Epoch [1/5], Step [269/782], Loss: 3.257228374481201\n",
            "Epoch [1/5], Step [270/782], Loss: 3.242170810699463\n",
            "Epoch [1/5], Step [271/782], Loss: 3.339142322540283\n",
            "Epoch [1/5], Step [272/782], Loss: 3.068406105041504\n",
            "Epoch [1/5], Step [273/782], Loss: 3.138762950897217\n",
            "Epoch [1/5], Step [274/782], Loss: 3.3246634006500244\n",
            "Epoch [1/5], Step [275/782], Loss: 3.081123113632202\n",
            "Epoch [1/5], Step [276/782], Loss: 3.3221349716186523\n",
            "Epoch [1/5], Step [277/782], Loss: 3.3377959728240967\n",
            "Epoch [1/5], Step [278/782], Loss: 3.24007248878479\n",
            "Epoch [1/5], Step [279/782], Loss: 3.221822738647461\n",
            "Epoch [1/5], Step [280/782], Loss: 3.0353081226348877\n",
            "Epoch [1/5], Step [281/782], Loss: 3.2133634090423584\n",
            "Epoch [1/5], Step [282/782], Loss: 3.333657741546631\n",
            "Epoch [1/5], Step [283/782], Loss: 3.2338991165161133\n",
            "Epoch [1/5], Step [284/782], Loss: 3.2112810611724854\n",
            "Epoch [1/5], Step [285/782], Loss: 2.9375832080841064\n",
            "Epoch [1/5], Step [286/782], Loss: 3.1074483394622803\n",
            "Epoch [1/5], Step [287/782], Loss: 3.139652729034424\n",
            "Epoch [1/5], Step [288/782], Loss: 3.1807026863098145\n",
            "Epoch [1/5], Step [289/782], Loss: 3.2491884231567383\n",
            "Epoch [1/5], Step [290/782], Loss: 3.284843921661377\n",
            "Epoch [1/5], Step [291/782], Loss: 3.303936719894409\n",
            "Epoch [1/5], Step [292/782], Loss: 3.24647855758667\n",
            "Epoch [1/5], Step [293/782], Loss: 3.2684507369995117\n",
            "Epoch [1/5], Step [294/782], Loss: 3.4448015689849854\n",
            "Epoch [1/5], Step [295/782], Loss: 3.1567776203155518\n",
            "Epoch [1/5], Step [296/782], Loss: 3.082019805908203\n",
            "Epoch [1/5], Step [297/782], Loss: 3.3204898834228516\n",
            "Epoch [1/5], Step [298/782], Loss: 3.4134719371795654\n",
            "Epoch [1/5], Step [299/782], Loss: 3.2582149505615234\n",
            "Epoch [1/5], Step [300/782], Loss: 3.302502155303955\n",
            "Epoch [1/5], Step [301/782], Loss: 3.1783201694488525\n",
            "Epoch [1/5], Step [302/782], Loss: 3.6088707447052\n",
            "Epoch [1/5], Step [303/782], Loss: 3.153303623199463\n",
            "Epoch [1/5], Step [304/782], Loss: 3.169590473175049\n",
            "Epoch [1/5], Step [305/782], Loss: 3.098754644393921\n",
            "Epoch [1/5], Step [306/782], Loss: 3.341805934906006\n",
            "Epoch [1/5], Step [307/782], Loss: 3.047544479370117\n",
            "Epoch [1/5], Step [308/782], Loss: 3.1562552452087402\n",
            "Epoch [1/5], Step [309/782], Loss: 3.256690740585327\n",
            "Epoch [1/5], Step [310/782], Loss: 3.1895430088043213\n",
            "Epoch [1/5], Step [311/782], Loss: 3.0446434020996094\n",
            "Epoch [1/5], Step [312/782], Loss: 3.0423834323883057\n",
            "Epoch [1/5], Step [313/782], Loss: 3.134199857711792\n",
            "Epoch [1/5], Step [314/782], Loss: 3.2302703857421875\n",
            "Epoch [1/5], Step [315/782], Loss: 3.0734570026397705\n",
            "Epoch [1/5], Step [316/782], Loss: 3.1955456733703613\n",
            "Epoch [1/5], Step [317/782], Loss: 3.14383602142334\n",
            "Epoch [1/5], Step [318/782], Loss: 3.1100382804870605\n",
            "Epoch [1/5], Step [319/782], Loss: 3.344961166381836\n",
            "Epoch [1/5], Step [320/782], Loss: 2.970313549041748\n",
            "Epoch [1/5], Step [321/782], Loss: 3.0186994075775146\n",
            "Epoch [1/5], Step [322/782], Loss: 3.025078296661377\n",
            "Epoch [1/5], Step [323/782], Loss: 3.2228922843933105\n",
            "Epoch [1/5], Step [324/782], Loss: 3.2172210216522217\n",
            "Epoch [1/5], Step [325/782], Loss: 2.9471564292907715\n",
            "Epoch [1/5], Step [326/782], Loss: 3.3034157752990723\n",
            "Epoch [1/5], Step [327/782], Loss: 3.0463736057281494\n",
            "Epoch [1/5], Step [328/782], Loss: 3.212078094482422\n",
            "Epoch [1/5], Step [329/782], Loss: 3.2639784812927246\n",
            "Epoch [1/5], Step [330/782], Loss: 3.053248882293701\n",
            "Epoch [1/5], Step [331/782], Loss: 2.9351284503936768\n",
            "Epoch [1/5], Step [332/782], Loss: 2.855797290802002\n",
            "Epoch [1/5], Step [333/782], Loss: 3.0205934047698975\n",
            "Epoch [1/5], Step [334/782], Loss: 2.9626643657684326\n",
            "Epoch [1/5], Step [335/782], Loss: 3.1036157608032227\n",
            "Epoch [1/5], Step [336/782], Loss: 3.004774808883667\n",
            "Epoch [1/5], Step [337/782], Loss: 3.10135555267334\n",
            "Epoch [1/5], Step [338/782], Loss: 3.16048526763916\n",
            "Epoch [1/5], Step [339/782], Loss: 2.8857481479644775\n",
            "Epoch [1/5], Step [340/782], Loss: 3.3083951473236084\n",
            "Epoch [1/5], Step [341/782], Loss: 3.0841617584228516\n",
            "Epoch [1/5], Step [342/782], Loss: 2.904113531112671\n",
            "Epoch [1/5], Step [343/782], Loss: 3.129647970199585\n",
            "Epoch [1/5], Step [344/782], Loss: 2.9152984619140625\n",
            "Epoch [1/5], Step [345/782], Loss: 2.9965014457702637\n",
            "Epoch [1/5], Step [346/782], Loss: 3.0462512969970703\n",
            "Epoch [1/5], Step [347/782], Loss: 3.0632541179656982\n",
            "Epoch [1/5], Step [348/782], Loss: 2.8263726234436035\n",
            "Epoch [1/5], Step [349/782], Loss: 3.2484562397003174\n",
            "Epoch [1/5], Step [350/782], Loss: 2.9460983276367188\n",
            "Epoch [1/5], Step [351/782], Loss: 3.243143081665039\n",
            "Epoch [1/5], Step [352/782], Loss: 2.9677505493164062\n",
            "Epoch [1/5], Step [353/782], Loss: 3.0664048194885254\n",
            "Epoch [1/5], Step [354/782], Loss: 2.980344295501709\n",
            "Epoch [1/5], Step [355/782], Loss: 2.788180351257324\n",
            "Epoch [1/5], Step [356/782], Loss: 3.091156244277954\n",
            "Epoch [1/5], Step [357/782], Loss: 3.214834213256836\n",
            "Epoch [1/5], Step [358/782], Loss: 3.105648994445801\n",
            "Epoch [1/5], Step [359/782], Loss: 2.937472343444824\n",
            "Epoch [1/5], Step [360/782], Loss: 2.8825180530548096\n",
            "Epoch [1/5], Step [361/782], Loss: 3.059255599975586\n",
            "Epoch [1/5], Step [362/782], Loss: 2.996659994125366\n",
            "Epoch [1/5], Step [363/782], Loss: 3.1017587184906006\n",
            "Epoch [1/5], Step [364/782], Loss: 2.9706521034240723\n",
            "Epoch [1/5], Step [365/782], Loss: 2.991079807281494\n",
            "Epoch [1/5], Step [366/782], Loss: 2.922790050506592\n",
            "Epoch [1/5], Step [367/782], Loss: 2.958142042160034\n",
            "Epoch [1/5], Step [368/782], Loss: 3.16864013671875\n",
            "Epoch [1/5], Step [369/782], Loss: 2.802687406539917\n",
            "Epoch [1/5], Step [370/782], Loss: 3.0360631942749023\n",
            "Epoch [1/5], Step [371/782], Loss: 3.1093664169311523\n",
            "Epoch [1/5], Step [372/782], Loss: 2.9750473499298096\n",
            "Epoch [1/5], Step [373/782], Loss: 3.098214626312256\n",
            "Epoch [1/5], Step [374/782], Loss: 3.0030322074890137\n",
            "Epoch [1/5], Step [375/782], Loss: 2.9786148071289062\n",
            "Epoch [1/5], Step [376/782], Loss: 2.89190411567688\n",
            "Epoch [1/5], Step [377/782], Loss: 2.984375\n",
            "Epoch [1/5], Step [378/782], Loss: 3.040093421936035\n",
            "Epoch [1/5], Step [379/782], Loss: 3.13854718208313\n",
            "Epoch [1/5], Step [380/782], Loss: 2.7008674144744873\n",
            "Epoch [1/5], Step [381/782], Loss: 2.9740991592407227\n",
            "Epoch [1/5], Step [382/782], Loss: 3.303718090057373\n",
            "Epoch [1/5], Step [383/782], Loss: 2.948652982711792\n",
            "Epoch [1/5], Step [384/782], Loss: 2.8791065216064453\n",
            "Epoch [1/5], Step [385/782], Loss: 2.9791083335876465\n",
            "Epoch [1/5], Step [386/782], Loss: 3.047682285308838\n",
            "Epoch [1/5], Step [387/782], Loss: 2.933743476867676\n",
            "Epoch [1/5], Step [388/782], Loss: 3.0117485523223877\n",
            "Epoch [1/5], Step [389/782], Loss: 2.88226318359375\n",
            "Epoch [1/5], Step [390/782], Loss: 3.000713348388672\n",
            "Epoch [1/5], Step [391/782], Loss: 3.092555522918701\n",
            "Epoch [1/5], Step [392/782], Loss: 3.189943313598633\n",
            "Epoch [1/5], Step [393/782], Loss: 2.9614474773406982\n",
            "Epoch [1/5], Step [394/782], Loss: 3.2534399032592773\n",
            "Epoch [1/5], Step [395/782], Loss: 2.8013834953308105\n",
            "Epoch [1/5], Step [396/782], Loss: 2.7636163234710693\n",
            "Epoch [1/5], Step [397/782], Loss: 2.8046865463256836\n",
            "Epoch [1/5], Step [398/782], Loss: 2.9531171321868896\n",
            "Epoch [1/5], Step [399/782], Loss: 2.8865718841552734\n",
            "Epoch [1/5], Step [400/782], Loss: 2.9661059379577637\n",
            "Epoch [1/5], Step [401/782], Loss: 3.247638463973999\n",
            "Epoch [1/5], Step [402/782], Loss: 2.903841972351074\n",
            "Epoch [1/5], Step [403/782], Loss: 2.9580774307250977\n",
            "Epoch [1/5], Step [404/782], Loss: 3.0503411293029785\n",
            "Epoch [1/5], Step [405/782], Loss: 3.124845027923584\n",
            "Epoch [1/5], Step [406/782], Loss: 3.035898208618164\n",
            "Epoch [1/5], Step [407/782], Loss: 2.8905022144317627\n",
            "Epoch [1/5], Step [408/782], Loss: 3.0271799564361572\n",
            "Epoch [1/5], Step [409/782], Loss: 3.137279510498047\n",
            "Epoch [1/5], Step [410/782], Loss: 2.9220080375671387\n",
            "Epoch [1/5], Step [411/782], Loss: 3.10848331451416\n",
            "Epoch [1/5], Step [412/782], Loss: 2.8170628547668457\n",
            "Epoch [1/5], Step [413/782], Loss: 2.9329147338867188\n",
            "Epoch [1/5], Step [414/782], Loss: 2.846097946166992\n",
            "Epoch [1/5], Step [415/782], Loss: 3.0441536903381348\n",
            "Epoch [1/5], Step [416/782], Loss: 2.7077202796936035\n",
            "Epoch [1/5], Step [417/782], Loss: 2.87520432472229\n",
            "Epoch [1/5], Step [418/782], Loss: 3.0214076042175293\n",
            "Epoch [1/5], Step [419/782], Loss: 2.8419508934020996\n",
            "Epoch [1/5], Step [420/782], Loss: 2.8852713108062744\n",
            "Epoch [1/5], Step [421/782], Loss: 3.1277084350585938\n",
            "Epoch [1/5], Step [422/782], Loss: 2.9920248985290527\n",
            "Epoch [1/5], Step [423/782], Loss: 2.7477152347564697\n",
            "Epoch [1/5], Step [424/782], Loss: 2.9192116260528564\n",
            "Epoch [1/5], Step [425/782], Loss: 2.966257095336914\n",
            "Epoch [1/5], Step [426/782], Loss: 2.869999647140503\n",
            "Epoch [1/5], Step [427/782], Loss: 3.0741207599639893\n",
            "Epoch [1/5], Step [428/782], Loss: 2.970898151397705\n",
            "Epoch [1/5], Step [429/782], Loss: 2.994309425354004\n",
            "Epoch [1/5], Step [430/782], Loss: 2.9578816890716553\n",
            "Epoch [1/5], Step [431/782], Loss: 2.9046077728271484\n",
            "Epoch [1/5], Step [432/782], Loss: 2.914989709854126\n",
            "Epoch [1/5], Step [433/782], Loss: 3.027913808822632\n",
            "Epoch [1/5], Step [434/782], Loss: 2.6418683528900146\n",
            "Epoch [1/5], Step [435/782], Loss: 2.936030387878418\n",
            "Epoch [1/5], Step [436/782], Loss: 2.7353179454803467\n",
            "Epoch [1/5], Step [437/782], Loss: 2.767258644104004\n",
            "Epoch [1/5], Step [438/782], Loss: 3.0729050636291504\n",
            "Epoch [1/5], Step [439/782], Loss: 2.8082306385040283\n",
            "Epoch [1/5], Step [440/782], Loss: 2.784816026687622\n",
            "Epoch [1/5], Step [441/782], Loss: 3.014561176300049\n",
            "Epoch [1/5], Step [442/782], Loss: 3.003026008605957\n",
            "Epoch [1/5], Step [443/782], Loss: 2.8541433811187744\n",
            "Epoch [1/5], Step [444/782], Loss: 2.8093013763427734\n",
            "Epoch [1/5], Step [445/782], Loss: 2.9884893894195557\n",
            "Epoch [1/5], Step [446/782], Loss: 2.9642751216888428\n",
            "Epoch [1/5], Step [447/782], Loss: 2.798103094100952\n",
            "Epoch [1/5], Step [448/782], Loss: 2.875051498413086\n",
            "Epoch [1/5], Step [449/782], Loss: 2.7006208896636963\n",
            "Epoch [1/5], Step [450/782], Loss: 2.8127591609954834\n",
            "Epoch [1/5], Step [451/782], Loss: 2.9904158115386963\n",
            "Epoch [1/5], Step [452/782], Loss: 2.9686312675476074\n",
            "Epoch [1/5], Step [453/782], Loss: 2.8591930866241455\n",
            "Epoch [1/5], Step [454/782], Loss: 2.8923439979553223\n",
            "Epoch [1/5], Step [455/782], Loss: 2.712862491607666\n",
            "Epoch [1/5], Step [456/782], Loss: 2.8706328868865967\n",
            "Epoch [1/5], Step [457/782], Loss: 2.7530417442321777\n",
            "Epoch [1/5], Step [458/782], Loss: 2.8443310260772705\n",
            "Epoch [1/5], Step [459/782], Loss: 2.9731152057647705\n",
            "Epoch [1/5], Step [460/782], Loss: 2.9210407733917236\n",
            "Epoch [1/5], Step [461/782], Loss: 2.569512367248535\n",
            "Epoch [1/5], Step [462/782], Loss: 2.826887607574463\n",
            "Epoch [1/5], Step [463/782], Loss: 2.8141772747039795\n",
            "Epoch [1/5], Step [464/782], Loss: 2.744105577468872\n",
            "Epoch [1/5], Step [465/782], Loss: 2.776085138320923\n",
            "Epoch [1/5], Step [466/782], Loss: 2.918264627456665\n",
            "Epoch [1/5], Step [467/782], Loss: 2.7643799781799316\n",
            "Epoch [1/5], Step [468/782], Loss: 2.67258358001709\n",
            "Epoch [1/5], Step [469/782], Loss: 2.679098129272461\n",
            "Epoch [1/5], Step [470/782], Loss: 2.787229061126709\n",
            "Epoch [1/5], Step [471/782], Loss: 2.7488901615142822\n",
            "Epoch [1/5], Step [472/782], Loss: 2.6116600036621094\n",
            "Epoch [1/5], Step [473/782], Loss: 2.8413472175598145\n",
            "Epoch [1/5], Step [474/782], Loss: 2.80940580368042\n",
            "Epoch [1/5], Step [475/782], Loss: 2.471243143081665\n",
            "Epoch [1/5], Step [476/782], Loss: 2.575669527053833\n",
            "Epoch [1/5], Step [477/782], Loss: 2.7128512859344482\n",
            "Epoch [1/5], Step [478/782], Loss: 2.896273136138916\n",
            "Epoch [1/5], Step [479/782], Loss: 2.6152522563934326\n",
            "Epoch [1/5], Step [480/782], Loss: 2.8695881366729736\n",
            "Epoch [1/5], Step [481/782], Loss: 2.539487361907959\n",
            "Epoch [1/5], Step [482/782], Loss: 2.6896727085113525\n",
            "Epoch [1/5], Step [483/782], Loss: 2.8529796600341797\n",
            "Epoch [1/5], Step [484/782], Loss: 2.6433377265930176\n",
            "Epoch [1/5], Step [485/782], Loss: 2.871568202972412\n",
            "Epoch [1/5], Step [486/782], Loss: 2.6964197158813477\n",
            "Epoch [1/5], Step [487/782], Loss: 3.131690740585327\n",
            "Epoch [1/5], Step [488/782], Loss: 2.8015546798706055\n",
            "Epoch [1/5], Step [489/782], Loss: 3.0441842079162598\n",
            "Epoch [1/5], Step [490/782], Loss: 3.0652570724487305\n",
            "Epoch [1/5], Step [491/782], Loss: 2.7902069091796875\n",
            "Epoch [1/5], Step [492/782], Loss: 2.895678997039795\n",
            "Epoch [1/5], Step [493/782], Loss: 2.744856834411621\n",
            "Epoch [1/5], Step [494/782], Loss: 2.9808692932128906\n",
            "Epoch [1/5], Step [495/782], Loss: 2.8323512077331543\n",
            "Epoch [1/5], Step [496/782], Loss: 2.628974437713623\n",
            "Epoch [1/5], Step [497/782], Loss: 2.9912962913513184\n",
            "Epoch [1/5], Step [498/782], Loss: 2.6587324142456055\n",
            "Epoch [1/5], Step [499/782], Loss: 2.756713390350342\n",
            "Epoch [1/5], Step [500/782], Loss: 2.8971803188323975\n",
            "Epoch [1/5], Step [501/782], Loss: 2.5494649410247803\n",
            "Epoch [1/5], Step [502/782], Loss: 2.7761051654815674\n",
            "Epoch [1/5], Step [503/782], Loss: 2.67785906791687\n",
            "Epoch [1/5], Step [504/782], Loss: 2.6020402908325195\n",
            "Epoch [1/5], Step [505/782], Loss: 2.917860746383667\n",
            "Epoch [1/5], Step [506/782], Loss: 2.721618413925171\n",
            "Epoch [1/5], Step [507/782], Loss: 2.6775803565979004\n",
            "Epoch [1/5], Step [508/782], Loss: 2.8914670944213867\n",
            "Epoch [1/5], Step [509/782], Loss: 2.758399724960327\n",
            "Epoch [1/5], Step [510/782], Loss: 2.669391632080078\n",
            "Epoch [1/5], Step [511/782], Loss: 2.981692314147949\n",
            "Epoch [1/5], Step [512/782], Loss: 2.4989402294158936\n",
            "Epoch [1/5], Step [513/782], Loss: 2.608011245727539\n",
            "Epoch [1/5], Step [514/782], Loss: 2.7115557193756104\n",
            "Epoch [1/5], Step [515/782], Loss: 2.503265380859375\n",
            "Epoch [1/5], Step [516/782], Loss: 2.685492515563965\n",
            "Epoch [1/5], Step [517/782], Loss: 2.6361072063446045\n",
            "Epoch [1/5], Step [518/782], Loss: 2.953378677368164\n",
            "Epoch [1/5], Step [519/782], Loss: 2.658740997314453\n",
            "Epoch [1/5], Step [520/782], Loss: 2.617476463317871\n",
            "Epoch [1/5], Step [521/782], Loss: 2.508202314376831\n",
            "Epoch [1/5], Step [522/782], Loss: 2.6487486362457275\n",
            "Epoch [1/5], Step [523/782], Loss: 2.5359458923339844\n",
            "Epoch [1/5], Step [524/782], Loss: 2.536884069442749\n",
            "Epoch [1/5], Step [525/782], Loss: 2.440802812576294\n",
            "Epoch [1/5], Step [526/782], Loss: 2.6141324043273926\n",
            "Epoch [1/5], Step [527/782], Loss: 2.8626346588134766\n",
            "Epoch [1/5], Step [528/782], Loss: 2.758087635040283\n",
            "Epoch [1/5], Step [529/782], Loss: 2.7471256256103516\n",
            "Epoch [1/5], Step [530/782], Loss: 2.8701891899108887\n",
            "Epoch [1/5], Step [531/782], Loss: 2.5784590244293213\n",
            "Epoch [1/5], Step [532/782], Loss: 2.573808431625366\n",
            "Epoch [1/5], Step [533/782], Loss: 2.631425380706787\n",
            "Epoch [1/5], Step [534/782], Loss: 2.825543165206909\n",
            "Epoch [1/5], Step [535/782], Loss: 2.6466903686523438\n",
            "Epoch [1/5], Step [536/782], Loss: 2.7626380920410156\n",
            "Epoch [1/5], Step [537/782], Loss: 2.431774616241455\n",
            "Epoch [1/5], Step [538/782], Loss: 2.518763542175293\n",
            "Epoch [1/5], Step [539/782], Loss: 2.6266558170318604\n",
            "Epoch [1/5], Step [540/782], Loss: 2.480314254760742\n",
            "Epoch [1/5], Step [541/782], Loss: 2.6229639053344727\n",
            "Epoch [1/5], Step [542/782], Loss: 2.89556622505188\n",
            "Epoch [1/5], Step [543/782], Loss: 2.5155491828918457\n",
            "Epoch [1/5], Step [544/782], Loss: 2.724351406097412\n",
            "Epoch [1/5], Step [545/782], Loss: 2.8534388542175293\n",
            "Epoch [1/5], Step [546/782], Loss: 2.576799154281616\n",
            "Epoch [1/5], Step [547/782], Loss: 2.4810619354248047\n",
            "Epoch [1/5], Step [548/782], Loss: 2.5758814811706543\n",
            "Epoch [1/5], Step [549/782], Loss: 2.5543837547302246\n",
            "Epoch [1/5], Step [550/782], Loss: 2.645610809326172\n",
            "Epoch [1/5], Step [551/782], Loss: 2.635615348815918\n",
            "Epoch [1/5], Step [552/782], Loss: 2.72119140625\n",
            "Epoch [1/5], Step [553/782], Loss: 2.7115345001220703\n",
            "Epoch [1/5], Step [554/782], Loss: 2.5752038955688477\n",
            "Epoch [1/5], Step [555/782], Loss: 2.5562829971313477\n",
            "Epoch [1/5], Step [556/782], Loss: 2.426769495010376\n",
            "Epoch [1/5], Step [557/782], Loss: 2.5603065490722656\n",
            "Epoch [1/5], Step [558/782], Loss: 2.493892192840576\n",
            "Epoch [1/5], Step [559/782], Loss: 2.8685295581817627\n",
            "Epoch [1/5], Step [560/782], Loss: 2.7428340911865234\n",
            "Epoch [1/5], Step [561/782], Loss: 2.7577028274536133\n",
            "Epoch [1/5], Step [562/782], Loss: 2.7051801681518555\n",
            "Epoch [1/5], Step [563/782], Loss: 2.5078012943267822\n",
            "Epoch [1/5], Step [564/782], Loss: 2.5946507453918457\n",
            "Epoch [1/5], Step [565/782], Loss: 2.7711806297302246\n",
            "Epoch [1/5], Step [566/782], Loss: 2.580920457839966\n",
            "Epoch [1/5], Step [567/782], Loss: 2.7074108123779297\n",
            "Epoch [1/5], Step [568/782], Loss: 2.441377878189087\n",
            "Epoch [1/5], Step [569/782], Loss: 2.4924888610839844\n",
            "Epoch [1/5], Step [570/782], Loss: 2.64298939704895\n",
            "Epoch [1/5], Step [571/782], Loss: 2.8313045501708984\n",
            "Epoch [1/5], Step [572/782], Loss: 2.714371681213379\n",
            "Epoch [1/5], Step [573/782], Loss: 2.594613790512085\n",
            "Epoch [1/5], Step [574/782], Loss: 2.442176342010498\n",
            "Epoch [1/5], Step [575/782], Loss: 2.449132204055786\n",
            "Epoch [1/5], Step [576/782], Loss: 2.4322590827941895\n",
            "Epoch [1/5], Step [577/782], Loss: 2.652123212814331\n",
            "Epoch [1/5], Step [578/782], Loss: 2.4951682090759277\n",
            "Epoch [1/5], Step [579/782], Loss: 2.5414371490478516\n",
            "Epoch [1/5], Step [580/782], Loss: 2.7178688049316406\n",
            "Epoch [1/5], Step [581/782], Loss: 2.824888229370117\n",
            "Epoch [1/5], Step [582/782], Loss: 2.715007781982422\n",
            "Epoch [1/5], Step [583/782], Loss: 2.4140734672546387\n",
            "Epoch [1/5], Step [584/782], Loss: 2.548084020614624\n",
            "Epoch [1/5], Step [585/782], Loss: 2.595606803894043\n",
            "Epoch [1/5], Step [586/782], Loss: 2.5944132804870605\n",
            "Epoch [1/5], Step [587/782], Loss: 2.972846508026123\n",
            "Epoch [1/5], Step [588/782], Loss: 2.518123149871826\n",
            "Epoch [1/5], Step [589/782], Loss: 2.564100980758667\n",
            "Epoch [1/5], Step [590/782], Loss: 2.6774163246154785\n",
            "Epoch [1/5], Step [591/782], Loss: 2.5429625511169434\n",
            "Epoch [1/5], Step [592/782], Loss: 2.7834980487823486\n",
            "Epoch [1/5], Step [593/782], Loss: 2.8611299991607666\n",
            "Epoch [1/5], Step [594/782], Loss: 2.4583985805511475\n",
            "Epoch [1/5], Step [595/782], Loss: 2.6560654640197754\n",
            "Epoch [1/5], Step [596/782], Loss: 2.5388200283050537\n",
            "Epoch [1/5], Step [597/782], Loss: 2.41985821723938\n",
            "Epoch [1/5], Step [598/782], Loss: 2.5443432331085205\n",
            "Epoch [1/5], Step [599/782], Loss: 2.810706615447998\n",
            "Epoch [1/5], Step [600/782], Loss: 2.7844767570495605\n",
            "Epoch [1/5], Step [601/782], Loss: 2.6593308448791504\n",
            "Epoch [1/5], Step [602/782], Loss: 2.79976487159729\n",
            "Epoch [1/5], Step [603/782], Loss: 2.5534400939941406\n",
            "Epoch [1/5], Step [604/782], Loss: 2.604104995727539\n",
            "Epoch [1/5], Step [605/782], Loss: 2.4746904373168945\n",
            "Epoch [1/5], Step [606/782], Loss: 2.5507614612579346\n",
            "Epoch [1/5], Step [607/782], Loss: 2.4663469791412354\n",
            "Epoch [1/5], Step [608/782], Loss: 2.6036441326141357\n",
            "Epoch [1/5], Step [609/782], Loss: 2.4960415363311768\n",
            "Epoch [1/5], Step [610/782], Loss: 2.5816431045532227\n",
            "Epoch [1/5], Step [611/782], Loss: 2.6957807540893555\n",
            "Epoch [1/5], Step [612/782], Loss: 2.392998695373535\n",
            "Epoch [1/5], Step [613/782], Loss: 2.6554291248321533\n",
            "Epoch [1/5], Step [614/782], Loss: 2.569225311279297\n",
            "Epoch [1/5], Step [615/782], Loss: 2.4321813583374023\n",
            "Epoch [1/5], Step [616/782], Loss: 2.4687068462371826\n",
            "Epoch [1/5], Step [617/782], Loss: 2.6439290046691895\n",
            "Epoch [1/5], Step [618/782], Loss: 2.551577568054199\n",
            "Epoch [1/5], Step [619/782], Loss: 2.403749942779541\n",
            "Epoch [1/5], Step [620/782], Loss: 2.4565658569335938\n",
            "Epoch [1/5], Step [621/782], Loss: 2.400545597076416\n",
            "Epoch [1/5], Step [622/782], Loss: 2.481515407562256\n",
            "Epoch [1/5], Step [623/782], Loss: 2.436309337615967\n",
            "Epoch [1/5], Step [624/782], Loss: 2.501603841781616\n",
            "Epoch [1/5], Step [625/782], Loss: 2.493638753890991\n",
            "Epoch [1/5], Step [626/782], Loss: 2.3100314140319824\n",
            "Epoch [1/5], Step [627/782], Loss: 2.7599422931671143\n",
            "Epoch [1/5], Step [628/782], Loss: 2.3545448780059814\n",
            "Epoch [1/5], Step [629/782], Loss: 2.584498405456543\n",
            "Epoch [1/5], Step [630/782], Loss: 2.823711633682251\n",
            "Epoch [1/5], Step [631/782], Loss: 2.4258430004119873\n",
            "Epoch [1/5], Step [632/782], Loss: 2.437323570251465\n",
            "Epoch [1/5], Step [633/782], Loss: 2.5956993103027344\n",
            "Epoch [1/5], Step [634/782], Loss: 2.363799571990967\n",
            "Epoch [1/5], Step [635/782], Loss: 2.4186344146728516\n",
            "Epoch [1/5], Step [636/782], Loss: 2.3545212745666504\n",
            "Epoch [1/5], Step [637/782], Loss: 2.507931709289551\n",
            "Epoch [1/5], Step [638/782], Loss: 2.5531771183013916\n",
            "Epoch [1/5], Step [639/782], Loss: 3.0967838764190674\n",
            "Epoch [1/5], Step [640/782], Loss: 2.6630945205688477\n",
            "Epoch [1/5], Step [641/782], Loss: 2.6284525394439697\n",
            "Epoch [1/5], Step [642/782], Loss: 2.570380687713623\n",
            "Epoch [1/5], Step [643/782], Loss: 2.48632550239563\n",
            "Epoch [1/5], Step [644/782], Loss: 2.642564535140991\n",
            "Epoch [1/5], Step [645/782], Loss: 2.4254376888275146\n",
            "Epoch [1/5], Step [646/782], Loss: 2.6665024757385254\n",
            "Epoch [1/5], Step [647/782], Loss: 2.7126030921936035\n",
            "Epoch [1/5], Step [648/782], Loss: 2.6476986408233643\n",
            "Epoch [1/5], Step [649/782], Loss: 2.4123375415802\n",
            "Epoch [1/5], Step [650/782], Loss: 2.518381118774414\n",
            "Epoch [1/5], Step [651/782], Loss: 2.3279552459716797\n",
            "Epoch [1/5], Step [652/782], Loss: 2.460272789001465\n",
            "Epoch [1/5], Step [653/782], Loss: 2.610191583633423\n",
            "Epoch [1/5], Step [654/782], Loss: 2.515866756439209\n",
            "Epoch [1/5], Step [655/782], Loss: 2.380293846130371\n",
            "Epoch [1/5], Step [656/782], Loss: 2.3553497791290283\n",
            "Epoch [1/5], Step [657/782], Loss: 2.6454052925109863\n",
            "Epoch [1/5], Step [658/782], Loss: 2.446748971939087\n",
            "Epoch [1/5], Step [659/782], Loss: 2.617608070373535\n",
            "Epoch [1/5], Step [660/782], Loss: 2.4227194786071777\n",
            "Epoch [1/5], Step [661/782], Loss: 2.422783851623535\n",
            "Epoch [1/5], Step [662/782], Loss: 2.3749518394470215\n",
            "Epoch [1/5], Step [663/782], Loss: 2.595269203186035\n",
            "Epoch [1/5], Step [664/782], Loss: 2.5846402645111084\n",
            "Epoch [1/5], Step [665/782], Loss: 2.6935174465179443\n",
            "Epoch [1/5], Step [666/782], Loss: 2.2096047401428223\n",
            "Epoch [1/5], Step [667/782], Loss: 2.263263702392578\n",
            "Epoch [1/5], Step [668/782], Loss: 2.3893163204193115\n",
            "Epoch [1/5], Step [669/782], Loss: 2.5833253860473633\n",
            "Epoch [1/5], Step [670/782], Loss: 2.413175106048584\n",
            "Epoch [1/5], Step [671/782], Loss: 2.2894678115844727\n",
            "Epoch [1/5], Step [672/782], Loss: 2.298494338989258\n",
            "Epoch [1/5], Step [673/782], Loss: 2.3010520935058594\n",
            "Epoch [1/5], Step [674/782], Loss: 2.351355791091919\n",
            "Epoch [1/5], Step [675/782], Loss: 2.2257237434387207\n",
            "Epoch [1/5], Step [676/782], Loss: 2.303571939468384\n",
            "Epoch [1/5], Step [677/782], Loss: 2.3130288124084473\n",
            "Epoch [1/5], Step [678/782], Loss: 2.4459831714630127\n",
            "Epoch [1/5], Step [679/782], Loss: 2.531345844268799\n",
            "Epoch [1/5], Step [680/782], Loss: 2.151564598083496\n",
            "Epoch [1/5], Step [681/782], Loss: 2.5871996879577637\n",
            "Epoch [1/5], Step [682/782], Loss: 2.2839314937591553\n",
            "Epoch [1/5], Step [683/782], Loss: 2.3673086166381836\n",
            "Epoch [1/5], Step [684/782], Loss: 2.218417167663574\n",
            "Epoch [1/5], Step [685/782], Loss: 2.371777057647705\n",
            "Epoch [1/5], Step [686/782], Loss: 2.180446147918701\n",
            "Epoch [1/5], Step [687/782], Loss: 2.2672290802001953\n",
            "Epoch [1/5], Step [688/782], Loss: 2.7110021114349365\n",
            "Epoch [1/5], Step [689/782], Loss: 2.4597551822662354\n",
            "Epoch [1/5], Step [690/782], Loss: 2.574455499649048\n",
            "Epoch [1/5], Step [691/782], Loss: 2.472487688064575\n",
            "Epoch [1/5], Step [692/782], Loss: 2.314863681793213\n",
            "Epoch [1/5], Step [693/782], Loss: 2.3166446685791016\n",
            "Epoch [1/5], Step [694/782], Loss: 2.366307258605957\n",
            "Epoch [1/5], Step [695/782], Loss: 2.3090438842773438\n",
            "Epoch [1/5], Step [696/782], Loss: 2.2211966514587402\n",
            "Epoch [1/5], Step [697/782], Loss: 2.580204486846924\n",
            "Epoch [1/5], Step [698/782], Loss: 2.421743154525757\n",
            "Epoch [1/5], Step [699/782], Loss: 2.391832113265991\n",
            "Epoch [1/5], Step [700/782], Loss: 2.3753645420074463\n",
            "Epoch [1/5], Step [701/782], Loss: 2.321093797683716\n",
            "Epoch [1/5], Step [702/782], Loss: 2.4584717750549316\n",
            "Epoch [1/5], Step [703/782], Loss: 2.4892451763153076\n",
            "Epoch [1/5], Step [704/782], Loss: 2.479762554168701\n",
            "Epoch [1/5], Step [705/782], Loss: 2.4090704917907715\n",
            "Epoch [1/5], Step [706/782], Loss: 2.245767116546631\n",
            "Epoch [1/5], Step [707/782], Loss: 2.2764077186584473\n",
            "Epoch [1/5], Step [708/782], Loss: 2.454582929611206\n",
            "Epoch [1/5], Step [709/782], Loss: 2.436976909637451\n",
            "Epoch [1/5], Step [710/782], Loss: 2.4736862182617188\n",
            "Epoch [1/5], Step [711/782], Loss: 2.306979179382324\n",
            "Epoch [1/5], Step [712/782], Loss: 2.4365360736846924\n",
            "Epoch [1/5], Step [713/782], Loss: 2.429856777191162\n",
            "Epoch [1/5], Step [714/782], Loss: 2.618232250213623\n",
            "Epoch [1/5], Step [715/782], Loss: 2.4120330810546875\n",
            "Epoch [1/5], Step [716/782], Loss: 2.2911062240600586\n",
            "Epoch [1/5], Step [717/782], Loss: 2.303762435913086\n",
            "Epoch [1/5], Step [718/782], Loss: 2.2343363761901855\n",
            "Epoch [1/5], Step [719/782], Loss: 2.2530033588409424\n",
            "Epoch [1/5], Step [720/782], Loss: 2.264857053756714\n",
            "Epoch [1/5], Step [721/782], Loss: 2.2947452068328857\n",
            "Epoch [1/5], Step [722/782], Loss: 2.3028440475463867\n",
            "Epoch [1/5], Step [723/782], Loss: 2.4980342388153076\n",
            "Epoch [1/5], Step [724/782], Loss: 2.185546875\n",
            "Epoch [1/5], Step [725/782], Loss: 2.447078227996826\n",
            "Epoch [1/5], Step [726/782], Loss: 2.2229766845703125\n",
            "Epoch [1/5], Step [727/782], Loss: 2.2588930130004883\n",
            "Epoch [1/5], Step [728/782], Loss: 2.201799154281616\n",
            "Epoch [1/5], Step [729/782], Loss: 2.162644863128662\n",
            "Epoch [1/5], Step [730/782], Loss: 2.148784637451172\n",
            "Epoch [1/5], Step [731/782], Loss: 2.180955171585083\n",
            "Epoch [1/5], Step [732/782], Loss: 2.214193820953369\n",
            "Epoch [1/5], Step [733/782], Loss: 2.4662890434265137\n",
            "Epoch [1/5], Step [734/782], Loss: 2.0798428058624268\n",
            "Epoch [1/5], Step [735/782], Loss: 2.2668774127960205\n",
            "Epoch [1/5], Step [736/782], Loss: 2.343465805053711\n",
            "Epoch [1/5], Step [737/782], Loss: 2.1571781635284424\n",
            "Epoch [1/5], Step [738/782], Loss: 2.302797317504883\n",
            "Epoch [1/5], Step [739/782], Loss: 2.6080496311187744\n",
            "Epoch [1/5], Step [740/782], Loss: 2.2707738876342773\n",
            "Epoch [1/5], Step [741/782], Loss: 2.1988840103149414\n",
            "Epoch [1/5], Step [742/782], Loss: 2.4688639640808105\n",
            "Epoch [1/5], Step [743/782], Loss: 2.45082426071167\n",
            "Epoch [1/5], Step [744/782], Loss: 2.4382028579711914\n",
            "Epoch [1/5], Step [745/782], Loss: 2.3064749240875244\n",
            "Epoch [1/5], Step [746/782], Loss: 2.5417397022247314\n",
            "Epoch [1/5], Step [747/782], Loss: 2.3300344944000244\n",
            "Epoch [1/5], Step [748/782], Loss: 2.2517805099487305\n",
            "Epoch [1/5], Step [749/782], Loss: 2.415731906890869\n",
            "Epoch [1/5], Step [750/782], Loss: 2.232631206512451\n",
            "Epoch [1/5], Step [751/782], Loss: 2.29390287399292\n",
            "Epoch [1/5], Step [752/782], Loss: 2.2049903869628906\n",
            "Epoch [1/5], Step [753/782], Loss: 2.1801464557647705\n",
            "Epoch [1/5], Step [754/782], Loss: 2.3484725952148438\n",
            "Epoch [1/5], Step [755/782], Loss: 2.3532776832580566\n",
            "Epoch [1/5], Step [756/782], Loss: 2.5518555641174316\n",
            "Epoch [1/5], Step [757/782], Loss: 2.179501533508301\n",
            "Epoch [1/5], Step [758/782], Loss: 2.4812068939208984\n",
            "Epoch [1/5], Step [759/782], Loss: 2.1990325450897217\n",
            "Epoch [1/5], Step [760/782], Loss: 2.2104170322418213\n",
            "Epoch [1/5], Step [761/782], Loss: 2.437835693359375\n",
            "Epoch [1/5], Step [762/782], Loss: 2.3445465564727783\n",
            "Epoch [1/5], Step [763/782], Loss: 2.564815044403076\n",
            "Epoch [1/5], Step [764/782], Loss: 2.2354040145874023\n",
            "Epoch [1/5], Step [765/782], Loss: 2.213475465774536\n",
            "Epoch [1/5], Step [766/782], Loss: 2.186278820037842\n",
            "Epoch [1/5], Step [767/782], Loss: 2.259794235229492\n",
            "Epoch [1/5], Step [768/782], Loss: 2.2358908653259277\n",
            "Epoch [1/5], Step [769/782], Loss: 2.227900981903076\n",
            "Epoch [1/5], Step [770/782], Loss: 2.3812615871429443\n",
            "Epoch [1/5], Step [771/782], Loss: 2.2586634159088135\n",
            "Epoch [1/5], Step [772/782], Loss: 2.1441221237182617\n",
            "Epoch [1/5], Step [773/782], Loss: 2.2828786373138428\n",
            "Epoch [1/5], Step [774/782], Loss: 2.135253667831421\n",
            "Epoch [1/5], Step [775/782], Loss: 2.3438262939453125\n",
            "Epoch [1/5], Step [776/782], Loss: 2.1852715015411377\n",
            "Epoch [1/5], Step [777/782], Loss: 2.4383745193481445\n",
            "Epoch [1/5], Step [778/782], Loss: 2.292947292327881\n",
            "Epoch [1/5], Step [779/782], Loss: 2.2420732975006104\n",
            "Epoch [1/5], Step [780/782], Loss: 2.3210792541503906\n",
            "Epoch [1/5], Step [781/782], Loss: 2.1935970783233643\n",
            "Epoch [1/5], Step [782/782], Loss: 2.229066848754883\n",
            "Epoch [2/5], Step [1/782], Loss: 2.237445592880249\n",
            "Epoch [2/5], Step [2/782], Loss: 2.4192750453948975\n",
            "Epoch [2/5], Step [3/782], Loss: 2.2100536823272705\n",
            "Epoch [2/5], Step [4/782], Loss: 2.292829751968384\n",
            "Epoch [2/5], Step [5/782], Loss: 2.3322229385375977\n",
            "Epoch [2/5], Step [6/782], Loss: 2.1816012859344482\n",
            "Epoch [2/5], Step [7/782], Loss: 2.2026917934417725\n",
            "Epoch [2/5], Step [8/782], Loss: 2.0294084548950195\n",
            "Epoch [2/5], Step [9/782], Loss: 2.3148391246795654\n",
            "Epoch [2/5], Step [10/782], Loss: 2.09019136428833\n",
            "Epoch [2/5], Step [11/782], Loss: 2.269272804260254\n",
            "Epoch [2/5], Step [12/782], Loss: 2.0543630123138428\n",
            "Epoch [2/5], Step [13/782], Loss: 2.2210919857025146\n",
            "Epoch [2/5], Step [14/782], Loss: 2.276735305786133\n",
            "Epoch [2/5], Step [15/782], Loss: 2.394146680831909\n",
            "Epoch [2/5], Step [16/782], Loss: 1.9836204051971436\n",
            "Epoch [2/5], Step [17/782], Loss: 2.1435153484344482\n",
            "Epoch [2/5], Step [18/782], Loss: 2.336740493774414\n",
            "Epoch [2/5], Step [19/782], Loss: 2.3377609252929688\n",
            "Epoch [2/5], Step [20/782], Loss: 2.2882590293884277\n",
            "Epoch [2/5], Step [21/782], Loss: 2.1804113388061523\n",
            "Epoch [2/5], Step [22/782], Loss: 2.0982213020324707\n",
            "Epoch [2/5], Step [23/782], Loss: 2.298159122467041\n",
            "Epoch [2/5], Step [24/782], Loss: 2.1847681999206543\n",
            "Epoch [2/5], Step [25/782], Loss: 2.1353609561920166\n",
            "Epoch [2/5], Step [26/782], Loss: 2.1727497577667236\n",
            "Epoch [2/5], Step [27/782], Loss: 2.2169322967529297\n",
            "Epoch [2/5], Step [28/782], Loss: 2.202270746231079\n",
            "Epoch [2/5], Step [29/782], Loss: 2.1135356426239014\n",
            "Epoch [2/5], Step [30/782], Loss: 2.401232957839966\n",
            "Epoch [2/5], Step [31/782], Loss: 2.3888132572174072\n",
            "Epoch [2/5], Step [32/782], Loss: 2.175161838531494\n",
            "Epoch [2/5], Step [33/782], Loss: 2.1638782024383545\n",
            "Epoch [2/5], Step [34/782], Loss: 2.3661253452301025\n",
            "Epoch [2/5], Step [35/782], Loss: 2.1492631435394287\n",
            "Epoch [2/5], Step [36/782], Loss: 2.2255449295043945\n",
            "Epoch [2/5], Step [37/782], Loss: 2.3215606212615967\n",
            "Epoch [2/5], Step [38/782], Loss: 2.2283992767333984\n",
            "Epoch [2/5], Step [39/782], Loss: 2.0909411907196045\n",
            "Epoch [2/5], Step [40/782], Loss: 2.055419445037842\n",
            "Epoch [2/5], Step [41/782], Loss: 2.128232717514038\n",
            "Epoch [2/5], Step [42/782], Loss: 2.2712297439575195\n",
            "Epoch [2/5], Step [43/782], Loss: 2.1335537433624268\n",
            "Epoch [2/5], Step [44/782], Loss: 2.1542983055114746\n",
            "Epoch [2/5], Step [45/782], Loss: 2.1723592281341553\n",
            "Epoch [2/5], Step [46/782], Loss: 2.1033473014831543\n",
            "Epoch [2/5], Step [47/782], Loss: 2.0471012592315674\n",
            "Epoch [2/5], Step [48/782], Loss: 2.0620229244232178\n",
            "Epoch [2/5], Step [49/782], Loss: 2.067821502685547\n",
            "Epoch [2/5], Step [50/782], Loss: 2.2163093090057373\n",
            "Epoch [2/5], Step [51/782], Loss: 2.1738381385803223\n",
            "Epoch [2/5], Step [52/782], Loss: 2.066990613937378\n",
            "Epoch [2/5], Step [53/782], Loss: 2.1080408096313477\n",
            "Epoch [2/5], Step [54/782], Loss: 2.245002269744873\n",
            "Epoch [2/5], Step [55/782], Loss: 2.1850786209106445\n",
            "Epoch [2/5], Step [56/782], Loss: 2.141556739807129\n",
            "Epoch [2/5], Step [57/782], Loss: 2.177219867706299\n",
            "Epoch [2/5], Step [58/782], Loss: 2.1814491748809814\n",
            "Epoch [2/5], Step [59/782], Loss: 2.0982842445373535\n",
            "Epoch [2/5], Step [60/782], Loss: 2.1080322265625\n",
            "Epoch [2/5], Step [61/782], Loss: 2.140165090560913\n",
            "Epoch [2/5], Step [62/782], Loss: 2.065512180328369\n",
            "Epoch [2/5], Step [63/782], Loss: 2.270095109939575\n",
            "Epoch [2/5], Step [64/782], Loss: 2.1056008338928223\n",
            "Epoch [2/5], Step [65/782], Loss: 2.4399569034576416\n",
            "Epoch [2/5], Step [66/782], Loss: 2.1296024322509766\n",
            "Epoch [2/5], Step [67/782], Loss: 2.224931240081787\n",
            "Epoch [2/5], Step [68/782], Loss: 2.226879358291626\n",
            "Epoch [2/5], Step [69/782], Loss: 2.112419605255127\n",
            "Epoch [2/5], Step [70/782], Loss: 2.051459550857544\n",
            "Epoch [2/5], Step [71/782], Loss: 2.0746426582336426\n",
            "Epoch [2/5], Step [72/782], Loss: 2.5201175212860107\n",
            "Epoch [2/5], Step [73/782], Loss: 2.0864791870117188\n",
            "Epoch [2/5], Step [74/782], Loss: 2.2000656127929688\n",
            "Epoch [2/5], Step [75/782], Loss: 2.171621322631836\n",
            "Epoch [2/5], Step [76/782], Loss: 1.8223892450332642\n",
            "Epoch [2/5], Step [77/782], Loss: 2.3194379806518555\n",
            "Epoch [2/5], Step [78/782], Loss: 2.0324044227600098\n",
            "Epoch [2/5], Step [79/782], Loss: 2.3287594318389893\n",
            "Epoch [2/5], Step [80/782], Loss: 2.149400234222412\n",
            "Epoch [2/5], Step [81/782], Loss: 1.9629820585250854\n",
            "Epoch [2/5], Step [82/782], Loss: 2.1513209342956543\n",
            "Epoch [2/5], Step [83/782], Loss: 1.954281210899353\n",
            "Epoch [2/5], Step [84/782], Loss: 1.970732569694519\n",
            "Epoch [2/5], Step [85/782], Loss: 2.0269908905029297\n",
            "Epoch [2/5], Step [86/782], Loss: 2.0490775108337402\n",
            "Epoch [2/5], Step [87/782], Loss: 2.347651958465576\n",
            "Epoch [2/5], Step [88/782], Loss: 2.1254382133483887\n",
            "Epoch [2/5], Step [89/782], Loss: 2.133150577545166\n",
            "Epoch [2/5], Step [90/782], Loss: 2.28707218170166\n",
            "Epoch [2/5], Step [91/782], Loss: 2.1602959632873535\n",
            "Epoch [2/5], Step [92/782], Loss: 1.997216820716858\n",
            "Epoch [2/5], Step [93/782], Loss: 2.2371931076049805\n",
            "Epoch [2/5], Step [94/782], Loss: 2.11291766166687\n",
            "Epoch [2/5], Step [95/782], Loss: 2.2836813926696777\n",
            "Epoch [2/5], Step [96/782], Loss: 2.153729200363159\n",
            "Epoch [2/5], Step [97/782], Loss: 2.1839215755462646\n",
            "Epoch [2/5], Step [98/782], Loss: 2.307413101196289\n",
            "Epoch [2/5], Step [99/782], Loss: 2.44586181640625\n",
            "Epoch [2/5], Step [100/782], Loss: 2.3746700286865234\n",
            "Epoch [2/5], Step [101/782], Loss: 2.118281126022339\n",
            "Epoch [2/5], Step [102/782], Loss: 2.2115325927734375\n",
            "Epoch [2/5], Step [103/782], Loss: 2.3811464309692383\n",
            "Epoch [2/5], Step [104/782], Loss: 2.091224193572998\n",
            "Epoch [2/5], Step [105/782], Loss: 2.17484188079834\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "# Training loop\n",
        "num_epochs = 5\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    # Inside the training loop\n",
        "    # Inside the training loop\n",
        "    # Inside the training loop\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        inception_outputs, _ = model(images)\n",
        "\n",
        "        # Ensure outputs and labels have the same batch size\n",
        "        labels = labels[:inception_outputs.shape[0]]  # Trim labels if needed to match batch size\n",
        "        loss = criterion(inception_outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Loss: {loss.item()}')\n",
        "\n",
        "\n",
        "        # Adjust learning rate\n",
        "    # scheduler.step()\n",
        "\n",
        "\"\"\"\n",
        "# Training loop\n",
        "num_epochs = 5\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        inception_outputs, _ = model(images)\n",
        "        labels = labels[:inception_outputs.shape[0]]\n",
        "        loss = criterion(inception_outputs, labels)\n",
        "\n",
        "        # Add L2 regularization term to the loss\n",
        "        l2_lambda = 0.001\n",
        "        l2_reg = sum(l2_lambda * p.norm(2) for p in model.parameters())\n",
        "        loss += l2_reg\n",
        "\n",
        "        # Backward and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Loss: {loss.item()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "506ec4b8-5e6e-40c3-94aa-0d439eae66a4",
      "metadata": {
        "id": "506ec4b8-5e6e-40c3-94aa-0d439eae66a4"
      },
      "outputs": [],
      "source": [
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTqY_F7vgB6M",
        "outputId": "6dbf090e-aa3a-49c8-b45a-34eecd295167"
      },
      "id": "TTqY_F7vgB6M",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f'Accuracy on the test set: {100 * accuracy:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XltMWrBchJ11",
        "outputId": "e64a3bbd-55a5-4311-9fcd-bb45dbee5f14"
      },
      "id": "XltMWrBchJ11",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the test set: 9.27%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_iyVyv9Kha_X"
      },
      "id": "_iyVyv9Kha_X",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}